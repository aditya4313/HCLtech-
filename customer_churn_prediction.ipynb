{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Machine Learning Model\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a complete Machine Learning pipeline to predict customer churn for an e-commerce business. The model identifies customers who are likely to discontinue using the company's services, enabling proactive retention strategies.\n",
    "\n",
    "## Objectives\n",
    "- Build a Machine Learning Prediction model to predict Customer Churn\n",
    "- Handle imbalanced datasets using SMOTE\n",
    "- Evaluate models using appropriate metrics\n",
    "- Generate Confusion Matrix and ROC Curve visualizations\n",
    "- Explain evaluation metrics and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "import kagglehub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "Download the dataset from Kaggle using `kagglehub`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Downloading Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"ankitverma2010/ecommerce-customer-churn-analysis-and-prediction\")\n",
    "    print(f\"✓ Dataset downloaded successfully!\")\n",
    "    print(f\"Path to dataset files: {path}\")\n",
    "    \n",
    "    # Find data files (CSV or Excel) in the downloaded directory\n",
    "    data_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.csv', '.xlsx', '.xls')):\n",
    "                data_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if data_files:\n",
    "        print(f\"\\nFound data files:\")\n",
    "        for data_file in data_files:\n",
    "            print(f\"  - {data_file}\")\n",
    "        file_path = data_files[0]  # Use first data file\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No CSV or Excel files found in downloaded dataset\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Data\n",
    "\n",
    "Load the dataset and explore its structure, missing values, and basic statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data based on file extension\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Loading and Exploring Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if file_path.endswith('.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "elif file_path.endswith(('.xlsx', '.xls')):\n",
    "    # Try to find the data sheet (skip metadata sheets)\n",
    "    xl_file = pd.ExcelFile(file_path)\n",
    "    sheet_names = xl_file.sheet_names\n",
    "    \n",
    "    # Look for common data sheet names or use the largest sheet\n",
    "    data_sheet = None\n",
    "    for sheet in sheet_names:\n",
    "        sheet_lower = sheet.lower()\n",
    "        # Skip metadata/dictionary sheets\n",
    "        if 'dict' not in sheet_lower and 'meta' not in sheet_lower and 'info' not in sheet_lower:\n",
    "            # Check if this sheet has substantial data\n",
    "            test_df = pd.read_excel(file_path, sheet_name=sheet, nrows=5)\n",
    "            if len(test_df.columns) > 2:  # Has multiple columns (likely data)\n",
    "                data_sheet = sheet\n",
    "                break\n",
    "    \n",
    "    # If no suitable sheet found, try the largest sheet\n",
    "    if data_sheet is None:\n",
    "        max_rows = 0\n",
    "        for sheet in sheet_names:\n",
    "            test_df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "            if len(test_df) > max_rows:\n",
    "                max_rows = len(test_df)\n",
    "                data_sheet = sheet\n",
    "    \n",
    "    # Load the data sheet\n",
    "    if data_sheet:\n",
    "        print(f\"\\nLoading data from sheet: '{data_sheet}'\")\n",
    "        df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
    "    else:\n",
    "        # Fallback to first sheet\n",
    "        print(f\"\\nWarning: Could not determine data sheet, using first sheet: '{sheet_names[0]}'\")\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_names[0])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning & Preprocessing\n",
    "\n",
    "Clean the data, handle missing values, encode categorical variables, and identify the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Data Cleaning & Preprocessing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "\n",
    "# Identify target variable\n",
    "target_candidates = ['churn', 'Churn', 'Churned', 'churned', 'is_churn']\n",
    "target_col = None\n",
    "for col in target_candidates:\n",
    "    if col in df_processed.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    for col in df_processed.columns:\n",
    "        if df_processed[col].dtype in ['int64', 'float64'] and df_processed[col].nunique() == 2:\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "print(f\"\\n✓ Target variable identified: {target_col}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(columns=[target_col])\n",
    "y = df_processed[target_col]\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "label_encoders = {}\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode target if needed\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "    label_encoders['target'] = le_target\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} ({c/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Data preprocessing completed!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Handle Imbalanced Dataset\n",
    "\n",
    "Apply SMOTE to balance the dataset and handle class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imbalanced dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Handling Imbalanced Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} ({c/len(y)*100:.2f}%)\")\n",
    "\n",
    "imbalance_ratio = min(counts) / max(counts)\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.3f}\")\n",
    "\n",
    "if imbalance_ratio < 0.5:\n",
    "    print(f\"\\n⚠ Dataset is imbalanced! Applying SMOTE technique...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\nAfter SMOTE:\")\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  Class {u}: {c} ({c/len(y_balanced)*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n✓ Dataset balanced successfully!\")\n",
    "else:\n",
    "    print(\"\\n✓ Dataset is relatively balanced, no resampling needed.\")\n",
    "    X_balanced, y_balanced = X, y\n",
    "\n",
    "print(f\"\\nBalanced dataset shape: {X_balanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split, Feature Scaling & PCA\n",
    "\n",
    "Split the data, scale features, and apply PCA for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split, scale, and apply PCA\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Train-Test Split, Feature Scaling & PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Original number of features: {X_balanced.shape[1]}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n✓ Data scaled successfully!\")\n",
    "\n",
    "# Apply PCA\n",
    "print(f\"\\nApplying PCA (target variance: 95.0%)...\")\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "n_components = pca.n_components_\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"✓ PCA applied successfully!\")\n",
    "print(f\"  Reduced dimensions: {X_balanced.shape[1]} → {n_components}\")\n",
    "print(f\"  Explained variance: {explained_variance*100:.2f}%\")\n",
    "print(f\"  Variance retained: {explained_variance*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nFinal feature dimensions:\")\n",
    "print(f\"  Training set: {X_train_scaled.shape}\")\n",
    "print(f\"  Test set: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Training with Cross-Validation (Random Forest)\n",
    "\n",
    "Train Random Forest model with GridSearchCV for hyperparameter tuning and cross-validation. Hyperparameters are tuned to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model with cross-validation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Model Training with Cross-Validation (Random Forest)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Random Forest model optimized for accuracy with minimal overfitting\n",
    "# Using RandomizedSearchCV for faster, more efficient hyperparameter search\n",
    "models_config = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 0.5, 1.0, 2.0],  # Regularization strength\n",
    "            'solver': ['lbfgs', 'liblinear']  # Solver algorithms\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'params': {\n",
    "            'n_estimators': [150, 200, 250],  # More trees for better accuracy\n",
    "            'max_depth': [6, 8, 10],  # Very shallow trees to minimize overfitting\n",
    "            'min_samples_split': [20, 30, 40],  # Very high threshold to prevent overfitting\n",
    "            'min_samples_leaf': [10, 15, 20],  # Very strong regularization\n",
    "            'max_features': ['sqrt', 'log2']  # Feature diversity\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds for speed\n",
    "target_accuracy_rf = 0.85\n",
    "target_accuracy_lr = 0.70\n",
    "results = {}\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # RandomizedSearchCV for faster hyperparameter tuning\n",
    "    print(\"  Performing RandomizedSearchCV for hyperparameter tuning...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        n_iter=15, \n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    print(f\"  Best parameters: {random_search.best_params_}\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    print(\"  Performing cross-validation...\")\n",
    "    cv_scores = cross_val_score(\n",
    "        best_model, X_train_scaled, y_train,\n",
    "        cv=cv, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    print(f\"  CV Accuracy: {cv_mean:.4f} (+/- {cv_std*2:.4f})\")\n",
    "    \n",
    "    # Train and test predictions\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Overfitting detection\n",
    "    overfitting_gap = train_accuracy - test_accuracy\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"  Overfitting Gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "    if overfitting_gap > 0.10:\n",
    "        print(f\"  ⚠ Warning: Potential overfitting detected (gap > 10%)\")\n",
    "    elif overfitting_gap > 0.05:\n",
    "        print(f\"  ⚠ Caution: Moderate overfitting (gap > 5%)\")\n",
    "    else:\n",
    "        print(f\"  ✓ Good generalization (overfitting gap < 5%)\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': best_model,\n",
    "        'accuracy': test_accuracy,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'best_params': random_search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final Test Metrics:\")\n",
    "    print(f\"    Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"    Precision: {precision:.4f}\")\n",
    "    print(f\"    Recall: {recall:.4f}\")\n",
    "    print(f\"    F1-Score: {f1:.4f}\")\n",
    "    print(f\"    ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Select Random Forest model\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Model Selection (Random Forest)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_model_name = 'Random Forest'\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ Selected Model: {best_model_name}\")\n",
    "print(f\"  Test Accuracy: {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)\")\n",
    "print(f\"  CV Mean Accuracy: {best_result['cv_mean']:.4f} (+/- {best_result['cv_std']*2:.4f})\")\n",
    "print(f\"  Overfitting Gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_result['roc_auc']:.4f}\")\n",
    "\n",
    "# Check overfitting and suggest improvements if needed\n",
    "if best_result['overfitting_gap'] > 0.05:\n",
    "    print(f\"\\n⚠ Overfitting detected! Consider:\")\n",
    "    print(f\"  - Further reducing max_depth\")\n",
    "    print(f\"  - Increasing min_samples_split and min_samples_leaf\")\n",
    "    print(f\"  - Using max_features='sqrt' or 'log2'\")\n",
    "\n",
    "# Check if target accuracy is achieved\n",
    "if abs(best_result['accuracy'] - target_accuracy) < 0.02:\n",
    "    print(f\"\\n✓ Target accuracy (~87%) achieved!\")\n",
    "else:\n",
    "    print(f\"\\n  Current accuracy: {best_result['accuracy']*100:.2f}%, Target: {target_accuracy*100:.2f}%\")\n",
    "\n",
    "# Store best model predictions\n",
    "y_pred = best_result['y_pred']\n",
    "y_pred_proba = best_result['y_pred_proba']\n",
    "\n",
    "# Save models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Saving Models\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = 'random_forest_model.pkl'\n",
    "joblib.dump(results['Random Forest']['model'], rf_model_path)\n",
    "print(f\"✓ Random Forest model saved: {rf_model_path}\")\n",
    "\n",
    "# Save Logistic Regression model\n",
    "lr_model_path = 'logistic_regression_model.pkl'\n",
    "joblib.dump(results['Logistic Regression']['model'], lr_model_path)\n",
    "print(f\"✓ Logistic Regression model saved: {lr_model_path}\")\n",
    "\n",
    "# Save scaler and PCA\n",
    "scaler_path = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Scaler saved: {scaler_path}\")\n",
    "\n",
    "if use_pca:\n",
    "    pca_path = 'pca.pkl'\n",
    "    joblib.dump(pca, pca_path)\n",
    "\n",
    "# Save label encoders\n",
    "if 'label_encoders' in locals() and label_encoders:\n",
    "    encoders_path = 'label_encoders.pkl'\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"✓ Label encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model': best_model_name,\n",
    "    'random_forest': {\n",
    "        'accuracy': results['Random Forest']['accuracy'],\n",
    "        'train_accuracy': results['Random Forest']['train_accuracy'],\n",
    "        'cv_mean': results['Random Forest']['cv_mean'],\n",
    "        'cv_std': results['Random Forest']['cv_std'],\n",
    "        'overfitting_gap': results['Random Forest']['overfitting_gap'],\n",
    "        'roc_auc': results['Random Forest']['roc_auc'],\n",
    "        'best_params': results['Random Forest']['best_params']\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'accuracy': results['Logistic Regression']['accuracy'],\n",
    "        'train_accuracy': results['Logistic Regression']['train_accuracy'],\n",
    "        'cv_mean': results['Logistic Regression']['cv_mean'],\n",
    "        'cv_std': results['Logistic Regression']['cv_std'],\n",
    "        'overfitting_gap': results['Logistic Regression']['overfitting_gap'],\n",
    "        'roc_auc': results['Logistic Regression']['roc_auc'],\n",
    "        'best_params': results['Logistic Regression']['best_params']\n",
    "    },\n",
    "    'pca_used': use_pca if 'use_pca' in locals() else False,\n",
    "    'pca_variance_retained': explained_variance if ('use_pca' in locals() and use_pca and 'explained_variance' in locals()) else None\n",
    "}\n",
    "metadata_path = 'model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"✓ Model metadata saved: {metadata_path}\")\n",
    "print(f\"✓ PCA saved: {pca_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation & Overfitting Analysis\n",
    "\n",
    "Evaluate the best model and analyze overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Model Evaluation & Overfitting Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "accuracy = best_result['accuracy']\n",
    "train_accuracy = best_result['train_accuracy']\n",
    "cv_mean = best_result['cv_mean']\n",
    "cv_std = best_result['cv_std']\n",
    "overfitting_gap = best_result['overfitting_gap']\n",
    "precision = best_result['precision']\n",
    "recall = best_result['recall']\n",
    "f1 = best_result['f1']\n",
    "roc_auc = best_result['roc_auc']\n",
    "\n",
    "print(f\"\\nModel Performance Metrics:\")\n",
    "print(f\"  Test Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Train Accuracy:    {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"  CV Mean Accuracy:  {cv_mean:.4f} (+/- {cv_std*2:.4f})\")\n",
    "print(f\"  Overfitting Gap:   {overfitting_gap:.4f}\")\n",
    "print(f\"  Precision:         {precision:.4f}\")\n",
    "print(f\"  Recall:            {recall:.4f}\")\n",
    "print(f\"  F1-Score:          {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:           {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nPCA Dimensionality Reduction:\")\n",
    "print(f\"  Original Features: {X.shape[1]}\")\n",
    "print(f\"  Reduced Features:  {n_components}\")\n",
    "print(f\"  Variance Retained: {explained_variance*100:.2f}%\")\n",
    "print(f\"  ✓ Features reduced while maintaining {explained_variance*100:.2f}% variance\")\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "if overfitting_gap < 0.02:\n",
    "    print(f\"  ✓ Excellent: No significant overfitting detected\")\n",
    "elif overfitting_gap < 0.05:\n",
    "    print(f\"  ✓ Good: Minimal overfitting (acceptable)\")\n",
    "elif overfitting_gap < 0.10:\n",
    "    print(f\"  ⚠ Caution: Moderate overfitting detected\")\n",
    "else:\n",
    "    print(f\"  ⚠ Warning: Significant overfitting detected\")\n",
    "\n",
    "print(f\"\\nCross-Validation Stability:\")\n",
    "if cv_std < 0.01:\n",
    "    print(f\"  ✓ Excellent: Very stable model (CV std < 1%)\")\n",
    "elif cv_std < 0.02:\n",
    "    print(f\"  ✓ Good: Stable model (CV std < 2%)\")\n",
    "else:\n",
    "    print(f\"  ⚠ Caution: Model variability detected (CV std >= 2%)\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Confusion Matrix\n",
    "\n",
    "Visualize the confusion matrix to understand model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 8: Generating Confusion Matrix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Churn', 'Churn'],\n",
    "            yticklabels=['Not Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Explain confusion matrix\n",
    "print(\"\\nConfusion Matrix Explanation:\")\n",
    "print(f\"  True Negatives (TN): {cm[0][0]} - Correctly predicted non-churn customers\")\n",
    "print(f\"  False Positives (FP): {cm[0][1]} - Incorrectly predicted as churn (Type I error)\")\n",
    "print(f\"  False Negatives (FN): {cm[1][0]} - Missed churn customers (Type II error)\")\n",
    "print(f\"  True Positives (TP): {cm[1][1]} - Correctly predicted churn customers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ROC Curve\n",
    "\n",
    "Generate and visualize the ROC curve to evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 9: Generating ROC Curve\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "        label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "        label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ ROC curve saved as 'roc_curve.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Explain ROC curve\n",
    "print(\"\\nROC Curve Explanation:\")\n",
    "print(f\"  AUC Score: {roc_auc:.4f}\")\n",
    "if roc_auc > 0.9:\n",
    "    print(\"  Interpretation: Excellent model performance\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"  Interpretation: Good model performance\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"  Interpretation: Moderate model performance\")\n",
    "else:\n",
    "    print(\"  Interpretation: Poor model performance - needs improvement\")\n",
    "print(\"\\n  The ROC curve shows the trade-off between:\")\n",
    "print(\"    - True Positive Rate (TPR): Ability to correctly identify churn customers\")\n",
    "print(\"    - False Positive Rate (FPR): Incorrectly flagging non-churn customers\")\n",
    "print(\"  A higher AUC indicates better model performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "Summary of the complete ML pipeline and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Train Accuracy:    {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"CV Mean Accuracy:  {cv_mean:.4f} ({cv_mean*100:.2f}%)\")\n",
    "print(f\"CV Std:            {cv_std:.4f} (±{cv_std*2:.4f})\")\n",
    "print(f\"Overfitting Gap:   {overfitting_gap:.4f}\")\n",
    "print(f\"Precision:         {precision:.4f}\")\n",
    "print(f\"Recall:            {recall:.4f}\")\n",
    "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nPCA Dimensionality Reduction:\")\n",
    "print(f\"  Original Features: {X.shape[1]}\")\n",
    "print(f\"  Reduced Features:  {n_components}\")\n",
    "print(f\"  Variance Retained: {explained_variance*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOverfitting Status:\")\n",
    "if overfitting_gap < 0.02:\n",
    "    print(\"  ✓ Excellent: No significant overfitting\")\n",
    "elif overfitting_gap < 0.05:\n",
    "    print(\"  ✓ Good: Minimal overfitting (acceptable)\")\n",
    "else:\n",
    "    print(\"  ⚠ Caution: Overfitting detected - model may not generalize well\")\n",
    "\n",
    "print(f\"\\nCross-Validation Stability:\")\n",
    "if cv_std < 0.01:\n",
    "    print(\"  ✓ Excellent: Very stable model\")\n",
    "elif cv_std < 0.02:\n",
    "    print(\"  ✓ Good: Stable model\")\n",
    "else:\n",
    "    print(\"  ⚠ Caution: Model variability detected\")\n",
    "\n",
    "target_accuracy = 0.87\n",
    "if abs(accuracy - target_accuracy) < 0.02:\n",
    "    print(f\"\\n✓ Target accuracy (~87%) achieved!\")\n",
    "else:\n",
    "    print(f\"\\n  Current accuracy: {accuracy*100:.2f}%, Target: {target_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
