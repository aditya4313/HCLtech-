{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Machine Learning Model\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a complete Machine Learning pipeline to predict customer churn for an e-commerce business. The model identifies customers who are likely to discontinue using the company's services, enabling proactive retention strategies.\n",
    "\n",
    "## Objectives\n",
    "- Build a Machine Learning Prediction model to predict Customer Churn\n",
    "- Handle imbalanced datasets using SMOTE\n",
    "- Evaluate models using appropriate metrics\n",
    "- Generate Confusion Matrix and ROC Curve visualizations\n",
    "- Explain evaluation metrics and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "import kagglehub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "Download the dataset from Kaggle using `kagglehub`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Downloading Dataset\n",
      "============================================================\n",
      "✓ Dataset downloaded successfully!\n",
      "Path to dataset files: C:\\Users\\gupta\\.cache\\kagglehub\\datasets\\ankitverma2010\\ecommerce-customer-churn-analysis-and-prediction\\versions\\1\n",
      "\n",
      "Found data files:\n",
      "  - C:\\Users\\gupta\\.cache\\kagglehub\\datasets\\ankitverma2010\\ecommerce-customer-churn-analysis-and-prediction\\versions\\1\\E Commerce Dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Kaggle\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Downloading Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"ankitverma2010/ecommerce-customer-churn-analysis-and-prediction\")\n",
    "    print(f\"✓ Dataset downloaded successfully!\")\n",
    "    print(f\"Path to dataset files: {path}\")\n",
    "    \n",
    "    # Find data files (CSV or Excel) in the downloaded directory\n",
    "    data_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.csv', '.xlsx', '.xls')):\n",
    "                data_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if data_files:\n",
    "        print(f\"\\nFound data files:\")\n",
    "        for data_file in data_files:\n",
    "            print(f\"  - {data_file}\")\n",
    "        file_path = data_files[0]  # Use first data file\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No CSV or Excel files found in downloaded dataset\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Data\n",
    "\n",
    "Load the dataset and explore its structure, missing values, and basic statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Loading and Exploring Data\n",
      "============================================================\n",
      "\n",
      "Loading data from sheet: 'E Comm'\n",
      "\n",
      "✓ Data loaded successfully!\n",
      "Dataset shape: (5630, 20)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>PreferredLoginDevice</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>WarehouseToHome</th>\n",
       "      <th>PreferredPaymentMode</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HourSpendOnApp</th>\n",
       "      <th>NumberOfDeviceRegistered</th>\n",
       "      <th>PreferedOrderCat</th>\n",
       "      <th>SatisfactionScore</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfAddress</th>\n",
       "      <th>Complain</th>\n",
       "      <th>OrderAmountHikeFromlastYear</th>\n",
       "      <th>CouponUsed</th>\n",
       "      <th>OrderCount</th>\n",
       "      <th>DaySinceLastOrder</th>\n",
       "      <th>CashbackAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50001</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mobile Phone</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Female</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laptop &amp; Accessory</td>\n",
       "      <td>2</td>\n",
       "      <td>Single</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>159.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>UPI</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50003</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Phone</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>Male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Laptop &amp; Accessory</td>\n",
       "      <td>5</td>\n",
       "      <td>Single</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>134.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>CC</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>5</td>\n",
       "      <td>Single</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>129.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Churn  Tenure PreferredLoginDevice  CityTier  WarehouseToHome  \\\n",
       "0       50001      1     4.0         Mobile Phone         3              6.0   \n",
       "1       50002      1     NaN                Phone         1              8.0   \n",
       "2       50003      1     NaN                Phone         1             30.0   \n",
       "3       50004      1     0.0                Phone         3             15.0   \n",
       "4       50005      1     0.0                Phone         1             12.0   \n",
       "\n",
       "  PreferredPaymentMode  Gender  HourSpendOnApp  NumberOfDeviceRegistered  \\\n",
       "0           Debit Card  Female             3.0                         3   \n",
       "1                  UPI    Male             3.0                         4   \n",
       "2           Debit Card    Male             2.0                         4   \n",
       "3           Debit Card    Male             2.0                         4   \n",
       "4                   CC    Male             NaN                         3   \n",
       "\n",
       "     PreferedOrderCat  SatisfactionScore MaritalStatus  NumberOfAddress  \\\n",
       "0  Laptop & Accessory                  2        Single                9   \n",
       "1              Mobile                  3        Single                7   \n",
       "2              Mobile                  3        Single                6   \n",
       "3  Laptop & Accessory                  5        Single                8   \n",
       "4              Mobile                  5        Single                3   \n",
       "\n",
       "   Complain  OrderAmountHikeFromlastYear  CouponUsed  OrderCount  \\\n",
       "0         1                         11.0         1.0         1.0   \n",
       "1         1                         15.0         0.0         1.0   \n",
       "2         1                         14.0         0.0         1.0   \n",
       "3         0                         23.0         0.0         1.0   \n",
       "4         0                         11.0         1.0         1.0   \n",
       "\n",
       "   DaySinceLastOrder  CashbackAmount  \n",
       "0                5.0          159.93  \n",
       "1                0.0          120.90  \n",
       "2                3.0          120.28  \n",
       "3                3.0          134.07  \n",
       "4                3.0          129.60  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data based on file extension\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Loading and Exploring Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if file_path.endswith('.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "elif file_path.endswith(('.xlsx', '.xls')):\n",
    "    # Try to find the data sheet (skip metadata sheets)\n",
    "    xl_file = pd.ExcelFile(file_path)\n",
    "    sheet_names = xl_file.sheet_names\n",
    "    \n",
    "    # Look for common data sheet names or use the largest sheet\n",
    "    data_sheet = None\n",
    "    for sheet in sheet_names:\n",
    "        sheet_lower = sheet.lower()\n",
    "        # Skip metadata/dictionary sheets\n",
    "        if 'dict' not in sheet_lower and 'meta' not in sheet_lower and 'info' not in sheet_lower:\n",
    "            # Check if this sheet has substantial data\n",
    "            test_df = pd.read_excel(file_path, sheet_name=sheet, nrows=5)\n",
    "            if len(test_df.columns) > 2:  # Has multiple columns (likely data)\n",
    "                data_sheet = sheet\n",
    "                break\n",
    "    \n",
    "    # If no suitable sheet found, try the largest sheet\n",
    "    if data_sheet is None:\n",
    "        max_rows = 0\n",
    "        for sheet in sheet_names:\n",
    "            test_df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "            if len(test_df) > max_rows:\n",
    "                max_rows = len(test_df)\n",
    "                data_sheet = sheet\n",
    "    \n",
    "    # Load the data sheet\n",
    "    if data_sheet:\n",
    "        print(f\"\\nLoading data from sheet: '{data_sheet}'\")\n",
    "        df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
    "    else:\n",
    "        # Fallback to first sheet\n",
    "        print(f\"\\nWarning: Could not determine data sheet, using first sheet: '{sheet_names[0]}'\")\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_names[0])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5630 entries, 0 to 5629\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   CustomerID                   5630 non-null   int64  \n",
      " 1   Churn                        5630 non-null   int64  \n",
      " 2   Tenure                       5366 non-null   float64\n",
      " 3   PreferredLoginDevice         5630 non-null   object \n",
      " 4   CityTier                     5630 non-null   int64  \n",
      " 5   WarehouseToHome              5379 non-null   float64\n",
      " 6   PreferredPaymentMode         5630 non-null   object \n",
      " 7   Gender                       5630 non-null   object \n",
      " 8   HourSpendOnApp               5375 non-null   float64\n",
      " 9   NumberOfDeviceRegistered     5630 non-null   int64  \n",
      " 10  PreferedOrderCat             5630 non-null   object \n",
      " 11  SatisfactionScore            5630 non-null   int64  \n",
      " 12  MaritalStatus                5630 non-null   object \n",
      " 13  NumberOfAddress              5630 non-null   int64  \n",
      " 14  Complain                     5630 non-null   int64  \n",
      " 15  OrderAmountHikeFromlastYear  5365 non-null   float64\n",
      " 16  CouponUsed                   5374 non-null   float64\n",
      " 17  OrderCount                   5372 non-null   float64\n",
      " 18  DaySinceLastOrder            5323 non-null   float64\n",
      " 19  CashbackAmount               5630 non-null   float64\n",
      "dtypes: float64(8), int64(7), object(5)\n",
      "memory usage: 879.8+ KB\n",
      "\n",
      "Missing values:\n",
      "CustomerID                       0\n",
      "Churn                            0\n",
      "Tenure                         264\n",
      "PreferredLoginDevice             0\n",
      "CityTier                         0\n",
      "WarehouseToHome                251\n",
      "PreferredPaymentMode             0\n",
      "Gender                           0\n",
      "HourSpendOnApp                 255\n",
      "NumberOfDeviceRegistered         0\n",
      "PreferedOrderCat                 0\n",
      "SatisfactionScore                0\n",
      "MaritalStatus                    0\n",
      "NumberOfAddress                  0\n",
      "Complain                         0\n",
      "OrderAmountHikeFromlastYear    265\n",
      "CouponUsed                     256\n",
      "OrderCount                     258\n",
      "DaySinceLastOrder              307\n",
      "CashbackAmount                   0\n",
      "dtype: int64\n",
      "\n",
      "Statistical Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>WarehouseToHome</th>\n",
       "      <th>HourSpendOnApp</th>\n",
       "      <th>NumberOfDeviceRegistered</th>\n",
       "      <th>SatisfactionScore</th>\n",
       "      <th>NumberOfAddress</th>\n",
       "      <th>Complain</th>\n",
       "      <th>OrderAmountHikeFromlastYear</th>\n",
       "      <th>CouponUsed</th>\n",
       "      <th>OrderCount</th>\n",
       "      <th>DaySinceLastOrder</th>\n",
       "      <th>CashbackAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5366.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5379.000000</td>\n",
       "      <td>5375.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>5365.000000</td>\n",
       "      <td>5374.000000</td>\n",
       "      <td>5372.000000</td>\n",
       "      <td>5323.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52815.500000</td>\n",
       "      <td>0.168384</td>\n",
       "      <td>10.189899</td>\n",
       "      <td>1.654707</td>\n",
       "      <td>15.639896</td>\n",
       "      <td>2.931535</td>\n",
       "      <td>3.688988</td>\n",
       "      <td>3.066785</td>\n",
       "      <td>4.214032</td>\n",
       "      <td>0.284902</td>\n",
       "      <td>15.707922</td>\n",
       "      <td>1.751023</td>\n",
       "      <td>3.008004</td>\n",
       "      <td>4.543491</td>\n",
       "      <td>177.223030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1625.385339</td>\n",
       "      <td>0.374240</td>\n",
       "      <td>8.557241</td>\n",
       "      <td>0.915389</td>\n",
       "      <td>8.531475</td>\n",
       "      <td>0.721926</td>\n",
       "      <td>1.023999</td>\n",
       "      <td>1.380194</td>\n",
       "      <td>2.583586</td>\n",
       "      <td>0.451408</td>\n",
       "      <td>3.675485</td>\n",
       "      <td>1.894621</td>\n",
       "      <td>2.939680</td>\n",
       "      <td>3.654433</td>\n",
       "      <td>49.207036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50001.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51408.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>145.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52815.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>163.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54222.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>196.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55630.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>324.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CustomerID        Churn       Tenure     CityTier  WarehouseToHome  \\\n",
       "count   5630.000000  5630.000000  5366.000000  5630.000000      5379.000000   \n",
       "mean   52815.500000     0.168384    10.189899     1.654707        15.639896   \n",
       "std     1625.385339     0.374240     8.557241     0.915389         8.531475   \n",
       "min    50001.000000     0.000000     0.000000     1.000000         5.000000   \n",
       "25%    51408.250000     0.000000     2.000000     1.000000         9.000000   \n",
       "50%    52815.500000     0.000000     9.000000     1.000000        14.000000   \n",
       "75%    54222.750000     0.000000    16.000000     3.000000        20.000000   \n",
       "max    55630.000000     1.000000    61.000000     3.000000       127.000000   \n",
       "\n",
       "       HourSpendOnApp  NumberOfDeviceRegistered  SatisfactionScore  \\\n",
       "count     5375.000000               5630.000000        5630.000000   \n",
       "mean         2.931535                  3.688988           3.066785   \n",
       "std          0.721926                  1.023999           1.380194   \n",
       "min          0.000000                  1.000000           1.000000   \n",
       "25%          2.000000                  3.000000           2.000000   \n",
       "50%          3.000000                  4.000000           3.000000   \n",
       "75%          3.000000                  4.000000           4.000000   \n",
       "max          5.000000                  6.000000           5.000000   \n",
       "\n",
       "       NumberOfAddress     Complain  OrderAmountHikeFromlastYear   CouponUsed  \\\n",
       "count      5630.000000  5630.000000                  5365.000000  5374.000000   \n",
       "mean          4.214032     0.284902                    15.707922     1.751023   \n",
       "std           2.583586     0.451408                     3.675485     1.894621   \n",
       "min           1.000000     0.000000                    11.000000     0.000000   \n",
       "25%           2.000000     0.000000                    13.000000     1.000000   \n",
       "50%           3.000000     0.000000                    15.000000     1.000000   \n",
       "75%           6.000000     1.000000                    18.000000     2.000000   \n",
       "max          22.000000     1.000000                    26.000000    16.000000   \n",
       "\n",
       "        OrderCount  DaySinceLastOrder  CashbackAmount  \n",
       "count  5372.000000        5323.000000     5630.000000  \n",
       "mean      3.008004           4.543491      177.223030  \n",
       "std       2.939680           3.654433       49.207036  \n",
       "min       1.000000           0.000000        0.000000  \n",
       "25%       1.000000           2.000000      145.770000  \n",
       "50%       2.000000           3.000000      163.280000  \n",
       "75%       3.000000           7.000000      196.392500  \n",
       "max      16.000000          46.000000      324.990000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning & Preprocessing\n",
    "\n",
    "Clean the data, handle missing values, encode categorical variables, and identify the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: Data Cleaning & Preprocessing\n",
      "============================================================\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "✓ Target variable identified: Churn\n",
      "\n",
      "Encoding categorical variables...\n",
      "\n",
      "Class distribution:\n",
      "  Class 0: 4682 (83.16%)\n",
      "  Class 1: 948 (16.84%)\n",
      "\n",
      "✓ Data preprocessing completed!\n",
      "Features shape: (5630, 19)\n",
      "Target shape: (5630,)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Data Cleaning & Preprocessing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "\n",
    "# Identify target variable\n",
    "target_candidates = ['churn', 'Churn', 'Churned', 'churned', 'is_churn']\n",
    "target_col = None\n",
    "for col in target_candidates:\n",
    "    if col in df_processed.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    for col in df_processed.columns:\n",
    "        if df_processed[col].dtype in ['int64', 'float64'] and df_processed[col].nunique() == 2:\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "print(f\"\\n✓ Target variable identified: {target_col}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(columns=[target_col])\n",
    "y = df_processed[target_col]\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "label_encoders = {}\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode target if needed\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "    label_encoders['target'] = le_target\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} ({c/len(y)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Data preprocessing completed!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Handle Imbalanced Dataset\n",
    "\n",
    "Apply SMOTE to balance the dataset and handle class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Handling Imbalanced Dataset\n",
      "============================================================\n",
      "\n",
      "Original class distribution:\n",
      "  Class 0: 4682 (83.16%)\n",
      "  Class 1: 948 (16.84%)\n",
      "\n",
      "Imbalance ratio: 0.202\n",
      "\n",
      "⚠ Dataset is imbalanced! Applying SMOTE technique...\n",
      "\n",
      "After SMOTE:\n",
      "  Class 0: 4682 (50.00%)\n",
      "  Class 1: 4682 (50.00%)\n",
      "\n",
      "✓ Dataset balanced successfully!\n",
      "\n",
      "Balanced dataset shape: (9364, 19)\n"
     ]
    }
   ],
   "source": [
    "# Handle imbalanced dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Handling Imbalanced Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nOriginal class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} ({c/len(y)*100:.2f}%)\")\n",
    "\n",
    "imbalance_ratio = min(counts) / max(counts)\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.3f}\")\n",
    "\n",
    "if imbalance_ratio < 0.5:\n",
    "    print(f\"\\n⚠ Dataset is imbalanced! Applying SMOTE technique...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\nAfter SMOTE:\")\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  Class {u}: {c} ({c/len(y_balanced)*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n✓ Dataset balanced successfully!\")\n",
    "else:\n",
    "    print(\"\\n✓ Dataset is relatively balanced, no resampling needed.\")\n",
    "    X_balanced, y_balanced = X, y\n",
    "\n",
    "print(f\"\\nBalanced dataset shape: {X_balanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split, Feature Scaling & PCA\n",
    "\n",
    "Split the data, scale features, and apply PCA for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: Train-Test Split, Feature Scaling & PCA\n",
      "============================================================\n",
      "\n",
      "Training set: (7491, 19)\n",
      "Test set: (1873, 19)\n",
      "Original number of features: 19\n",
      "\n",
      "✓ Data scaled successfully!\n",
      "\n",
      "Applying PCA (target variance: 95.0%)...\n",
      "✓ PCA applied successfully!\n",
      "  Reduced dimensions: 19 → 17\n",
      "  Explained variance: 96.65%\n",
      "  Variance retained: 96.65%\n",
      "\n",
      "Final feature dimensions:\n",
      "  Training set: (7491, 17)\n",
      "  Test set: (1873, 17)\n"
     ]
    }
   ],
   "source": [
    "# Split, scale, and apply PCA\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Train-Test Split, Feature Scaling & PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Original number of features: {X_balanced.shape[1]}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n✓ Data scaled successfully!\")\n",
    "\n",
    "# Apply PCA\n",
    "print(f\"\\nApplying PCA (target variance: 95.0%)...\")\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "n_components = pca.n_components_\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"✓ PCA applied successfully!\")\n",
    "print(f\"  Reduced dimensions: {X_balanced.shape[1]} → {n_components}\")\n",
    "print(f\"  Explained variance: {explained_variance*100:.2f}%\")\n",
    "print(f\"  Variance retained: {explained_variance*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nFinal feature dimensions:\")\n",
    "print(f\"  Training set: {X_train_scaled.shape}\")\n",
    "print(f\"  Test set: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Training with Cross-Validation (Random Forest)\n",
    "\n",
    "Train Random Forest model with GridSearchCV for hyperparameter tuning and cross-validation. Hyperparameters are tuned to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: Model Training with Cross-Validation (Random Forest)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Logistic Regression...\n",
      "============================================================\n",
      "  Performing RandomizedSearchCV for hyperparameter tuning...\n",
      "  Best parameters: {'solver': 'liblinear', 'C': 1.0}\n",
      "  Performing cross-validation...\n",
      "  CV Accuracy: 0.7800 (+/- 0.0056)\n",
      "  Train Accuracy: 0.7801\n",
      "  Test Accuracy:  0.7784\n",
      "  Overfitting Gap: 0.0017\n",
      "  ✓ Good generalization (overfitting gap < 5%)\n",
      "  Final Test Metrics:\n",
      "    Accuracy: 0.7784\n",
      "    Precision: 0.7816\n",
      "    Recall: 0.7784\n",
      "    F1-Score: 0.7778\n",
      "    ROC-AUC: 0.8534\n",
      "\n",
      "============================================================\n",
      "Training Random Forest...\n",
      "============================================================\n",
      "  Performing RandomizedSearchCV for hyperparameter tuning...\n",
      "  Best parameters: {'n_estimators': 200, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "  Performing cross-validation...\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest model with cross-validation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Model Training with Cross-Validation (Random Forest)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define Random Forest model optimized for accuracy with minimal overfitting\n",
    "# Using RandomizedSearchCV for faster, more efficient hyperparameter search\n",
    "models_config = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 0.5, 1.0, 2.0],  # Regularization strength\n",
    "            'solver': ['lbfgs', 'liblinear']  # Solver algorithms\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'params': {\n",
    "            'n_estimators': [150, 200, 250],  # More trees for better accuracy\n",
    "            'max_depth': [6, 8, 10],  # Very shallow trees to minimize overfitting\n",
    "            'min_samples_split': [20, 30, 40],  # Very high threshold to prevent overfitting\n",
    "            'min_samples_leaf': [10, 15, 20],  # Very strong regularization\n",
    "            'max_features': ['sqrt', 'log2']  # Feature diversity\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds for speed\n",
    "target_accuracy_rf = 0.85\n",
    "target_accuracy_lr = 0.70\n",
    "results = {}\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # RandomizedSearchCV for faster hyperparameter tuning\n",
    "    print(\"  Performing RandomizedSearchCV for hyperparameter tuning...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        n_iter=15, \n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    print(f\"  Best parameters: {random_search.best_params_}\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    print(\"  Performing cross-validation...\")\n",
    "    cv_scores = cross_val_score(\n",
    "        best_model, X_train_scaled, y_train,\n",
    "        cv=cv, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    print(f\"  CV Accuracy: {cv_mean:.4f} (+/- {cv_std*2:.4f})\")\n",
    "    \n",
    "    # Train and test predictions\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Overfitting detection\n",
    "    overfitting_gap = train_accuracy - test_accuracy\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"  Overfitting Gap: {overfitting_gap:.4f}\")\n",
    "    \n",
    "    if overfitting_gap > 0.10:\n",
    "        print(f\"  ⚠ Warning: Potential overfitting detected (gap > 10%)\")\n",
    "    elif overfitting_gap > 0.05:\n",
    "        print(f\"  ⚠ Caution: Moderate overfitting (gap > 5%)\")\n",
    "    else:\n",
    "        print(f\"  ✓ Good generalization (overfitting gap < 5%)\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': best_model,\n",
    "        'accuracy': test_accuracy,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'best_params': random_search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final Test Metrics:\")\n",
    "    print(f\"    Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"    Precision: {precision:.4f}\")\n",
    "    print(f\"    Recall: {recall:.4f}\")\n",
    "    print(f\"    F1-Score: {f1:.4f}\")\n",
    "    print(f\"    ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Select Random Forest model\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Model Selection (Random Forest)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_model_name = 'Random Forest'\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ Selected Model: {best_model_name}\")\n",
    "print(f\"  Test Accuracy: {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)\")\n",
    "print(f\"  CV Mean Accuracy: {best_result['cv_mean']:.4f} (+/- {best_result['cv_std']*2:.4f})\")\n",
    "print(f\"  Overfitting Gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_result['roc_auc']:.4f}\")\n",
    "\n",
    "# Check overfitting and suggest improvements if needed\n",
    "if best_result['overfitting_gap'] > 0.05:\n",
    "    print(f\"\\n⚠ Overfitting detected! Consider:\")\n",
    "    print(f\"  - Further reducing max_depth\")\n",
    "    print(f\"  - Increasing min_samples_split and min_samples_leaf\")\n",
    "    print(f\"  - Using max_features='sqrt' or 'log2'\")\n",
    "\n",
    "# Check if target accuracy is achieved\n",
    "if abs(best_result['accuracy'] - target_accuracy) < 0.02:\n",
    "    print(f\"\\n✓ Target accuracy (~87%) achieved!\")\n",
    "else:\n",
    "    print(f\"\\n  Current accuracy: {best_result['accuracy']*100:.2f}%, Target: {target_accuracy*100:.2f}%\")\n",
    "\n",
    "# Store best model predictions\n",
    "y_pred = best_result['y_pred']\n",
    "y_pred_proba = best_result['y_pred_proba']\n",
    "\n",
    "# Save models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Saving Models\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = 'random_forest_model.pkl'\n",
    "joblib.dump(results['Random Forest']['model'], rf_model_path)\n",
    "print(f\"✓ Random Forest model saved: {rf_model_path}\")\n",
    "\n",
    "# Save Logistic Regression model\n",
    "lr_model_path = 'logistic_regression_model.pkl'\n",
    "joblib.dump(results['Logistic Regression']['model'], lr_model_path)\n",
    "print(f\"✓ Logistic Regression model saved: {lr_model_path}\")\n",
    "\n",
    "# Save scaler and PCA\n",
    "scaler_path = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Scaler saved: {scaler_path}\")\n",
    "\n",
    "if use_pca:\n",
    "    pca_path = 'pca.pkl'\n",
    "    joblib.dump(pca, pca_path)\n",
    "\n",
    "# Save label encoders\n",
    "if 'label_encoders' in locals() and label_encoders:\n",
    "    encoders_path = 'label_encoders.pkl'\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"✓ Label encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model': best_model_name,\n",
    "    'random_forest': {\n",
    "        'accuracy': results['Random Forest']['accuracy'],\n",
    "        'train_accuracy': results['Random Forest']['train_accuracy'],\n",
    "        'cv_mean': results['Random Forest']['cv_mean'],\n",
    "        'cv_std': results['Random Forest']['cv_std'],\n",
    "        'overfitting_gap': results['Random Forest']['overfitting_gap'],\n",
    "        'roc_auc': results['Random Forest']['roc_auc'],\n",
    "        'best_params': results['Random Forest']['best_params']\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'accuracy': results['Logistic Regression']['accuracy'],\n",
    "        'train_accuracy': results['Logistic Regression']['train_accuracy'],\n",
    "        'cv_mean': results['Logistic Regression']['cv_mean'],\n",
    "        'cv_std': results['Logistic Regression']['cv_std'],\n",
    "        'overfitting_gap': results['Logistic Regression']['overfitting_gap'],\n",
    "        'roc_auc': results['Logistic Regression']['roc_auc'],\n",
    "        'best_params': results['Logistic Regression']['best_params']\n",
    "    },\n",
    "    'pca_used': use_pca if 'use_pca' in locals() else False,\n",
    "    'pca_variance_retained': explained_variance if ('use_pca' in locals() and use_pca and 'explained_variance' in locals()) else None\n",
    "}\n",
    "metadata_path = 'model_metadata.pkl'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"✓ Model metadata saved: {metadata_path}\")\n",
    "print(f\"✓ PCA saved: {pca_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation & Overfitting Analysis\n",
    "\n",
    "Evaluate the best model and analyze overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Model Evaluation & Overfitting Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "accuracy = best_result['accuracy']\n",
    "train_accuracy = best_result['train_accuracy']\n",
    "cv_mean = best_result['cv_mean']\n",
    "cv_std = best_result['cv_std']\n",
    "overfitting_gap = best_result['overfitting_gap']\n",
    "precision = best_result['precision']\n",
    "recall = best_result['recall']\n",
    "f1 = best_result['f1']\n",
    "roc_auc = best_result['roc_auc']\n",
    "\n",
    "print(f\"\\nModel Performance Metrics:\")\n",
    "print(f\"  Test Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Train Accuracy:    {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"  CV Mean Accuracy:  {cv_mean:.4f} (+/- {cv_std*2:.4f})\")\n",
    "print(f\"  Overfitting Gap:   {overfitting_gap:.4f}\")\n",
    "print(f\"  Precision:         {precision:.4f}\")\n",
    "print(f\"  Recall:            {recall:.4f}\")\n",
    "print(f\"  F1-Score:          {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:           {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nPCA Dimensionality Reduction:\")\n",
    "print(f\"  Original Features: {X.shape[1]}\")\n",
    "print(f\"  Reduced Features:  {n_components}\")\n",
    "print(f\"  Variance Retained: {explained_variance*100:.2f}%\")\n",
    "print(f\"  ✓ Features reduced while maintaining {explained_variance*100:.2f}% variance\")\n",
    "\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "if overfitting_gap < 0.02:\n",
    "    print(f\"  ✓ Excellent: No significant overfitting detected\")\n",
    "elif overfitting_gap < 0.05:\n",
    "    print(f\"  ✓ Good: Minimal overfitting (acceptable)\")\n",
    "elif overfitting_gap < 0.10:\n",
    "    print(f\"  ⚠ Caution: Moderate overfitting detected\")\n",
    "else:\n",
    "    print(f\"  ⚠ Warning: Significant overfitting detected\")\n",
    "\n",
    "print(f\"\\nCross-Validation Stability:\")\n",
    "if cv_std < 0.01:\n",
    "    print(f\"  ✓ Excellent: Very stable model (CV std < 1%)\")\n",
    "elif cv_std < 0.02:\n",
    "    print(f\"  ✓ Good: Stable model (CV std < 2%)\")\n",
    "else:\n",
    "    print(f\"  ⚠ Caution: Model variability detected (CV std >= 2%)\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Confusion Matrix\n",
    "\n",
    "Visualize the confusion matrix to understand model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 8: Generating Confusion Matrix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Churn', 'Churn'],\n",
    "            yticklabels=['Not Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Explain confusion matrix\n",
    "print(\"\\nConfusion Matrix Explanation:\")\n",
    "print(f\"  True Negatives (TN): {cm[0][0]} - Correctly predicted non-churn customers\")\n",
    "print(f\"  False Positives (FP): {cm[0][1]} - Incorrectly predicted as churn (Type I error)\")\n",
    "print(f\"  False Negatives (FN): {cm[1][0]} - Missed churn customers (Type II error)\")\n",
    "print(f\"  True Positives (TP): {cm[1][1]} - Correctly predicted churn customers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ROC Curve\n",
    "\n",
    "Generate and visualize the ROC curve to evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 9: Generating ROC Curve\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "        label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "        label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ ROC curve saved as 'roc_curve.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Explain ROC curve\n",
    "print(\"\\nROC Curve Explanation:\")\n",
    "print(f\"  AUC Score: {roc_auc:.4f}\")\n",
    "if roc_auc > 0.9:\n",
    "    print(\"  Interpretation: Excellent model performance\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"  Interpretation: Good model performance\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"  Interpretation: Moderate model performance\")\n",
    "else:\n",
    "    print(\"  Interpretation: Poor model performance - needs improvement\")\n",
    "print(\"\\n  The ROC curve shows the trade-off between:\")\n",
    "print(\"    - True Positive Rate (TPR): Ability to correctly identify churn customers\")\n",
    "print(\"    - False Positive Rate (FPR): Incorrectly flagging non-churn customers\")\n",
    "print(\"  A higher AUC indicates better model performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "Summary of the complete ML pipeline and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Train Accuracy:    {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"CV Mean Accuracy:  {cv_mean:.4f} ({cv_mean*100:.2f}%)\")\n",
    "print(f\"CV Std:            {cv_std:.4f} (±{cv_std*2:.4f})\")\n",
    "print(f\"Overfitting Gap:   {overfitting_gap:.4f}\")\n",
    "print(f\"Precision:         {precision:.4f}\")\n",
    "print(f\"Recall:            {recall:.4f}\")\n",
    "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nPCA Dimensionality Reduction:\")\n",
    "print(f\"  Original Features: {X.shape[1]}\")\n",
    "print(f\"  Reduced Features:  {n_components}\")\n",
    "print(f\"  Variance Retained: {explained_variance*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOverfitting Status:\")\n",
    "if overfitting_gap < 0.02:\n",
    "    print(\"  ✓ Excellent: No significant overfitting\")\n",
    "elif overfitting_gap < 0.05:\n",
    "    print(\"  ✓ Good: Minimal overfitting (acceptable)\")\n",
    "else:\n",
    "    print(\"  ⚠ Caution: Overfitting detected - model may not generalize well\")\n",
    "\n",
    "print(f\"\\nCross-Validation Stability:\")\n",
    "if cv_std < 0.01:\n",
    "    print(\"  ✓ Excellent: Very stable model\")\n",
    "elif cv_std < 0.02:\n",
    "    print(\"  ✓ Good: Stable model\")\n",
    "else:\n",
    "    print(\"  ⚠ Caution: Model variability detected\")\n",
    "\n",
    "target_accuracy = 0.87\n",
    "if abs(accuracy - target_accuracy) < 0.02:\n",
    "    print(f\"\\n✓ Target accuracy (~87%) achieved!\")\n",
    "else:\n",
    "    print(f\"\\n  Current accuracy: {accuracy*100:.2f}%, Target: {target_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
