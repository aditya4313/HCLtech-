"""
Customer Churn Prediction - Streamlit Web Application
A professional and minimal frontend for the ML model
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report
)
from imblearn.over_sampling import SMOTE
import kagglehub
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="Customer Churn Prediction",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for minimal, professional design
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 300;
        color: #1f1f1f;
        margin-bottom: 0.5rem;
        letter-spacing: -0.02em;
    }
    .sub-header {
        font-size: 1rem;
        color: #666;
        margin-bottom: 2rem;
        font-weight: 400;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 4px solid #0066cc;
    }
    .stButton>button {
        width: 100%;
        background-color: #0066cc;
        color: white;
        border-radius: 6px;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: 500;
        transition: all 0.3s;
    }
    .stButton>button:hover {
        background-color: #0052a3;
        transform: translateY(-1px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .success-box {
        padding: 1rem;
        background-color: #d4edda;
        border-left: 4px solid #28a745;
        border-radius: 4px;
        margin: 1rem 0;
    }
    </style>
""", unsafe_allow_html=True)

# Initialize session state
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'predictor' not in st.session_state:
    st.session_state.predictor = None


class ChurnPredictor:
    """Churn Prediction Model Class"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = None
        self.label_encoders = {}
        self.model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.y_pred = None
        self.y_pred_proba = None
        self.results = {}
        self.use_pca = True
        self.n_components = None
        self.explained_variance_ratio_ = None
        
    def download_data(self):
        """Download dataset from Kaggle"""
        try:
            path = kagglehub.dataset_download("ankitverma2010/ecommerce-customer-churn-analysis-and-prediction")
            
            # Find data files
            data_files = []
            for root, dirs, files in os.walk(path):
                for file in files:
                    if file.endswith(('.csv', '.xlsx', '.xls')):
                        data_files.append(os.path.join(root, file))
            
            return data_files[0] if data_files else None
        except Exception as e:
            st.error(f"Error downloading dataset: {e}")
            return None
    
    def load_data(self, file_path):
        """Load data from file"""
        if file_path.endswith('.csv'):
            return pd.read_csv(file_path)
        elif file_path.endswith(('.xlsx', '.xls')):
            xl_file = pd.ExcelFile(file_path)
            sheet_names = xl_file.sheet_names
            
            # Find data sheet
            data_sheet = None
            for sheet in sheet_names:
                sheet_lower = sheet.lower()
                if 'dict' not in sheet_lower and 'meta' not in sheet_lower:
                    test_df = pd.read_excel(file_path, sheet_name=sheet, nrows=5)
                    if len(test_df.columns) > 2:
                        data_sheet = sheet
                        break
            
            if data_sheet is None:
                max_rows = 0
                for sheet in sheet_names:
                    test_df = pd.read_excel(file_path, sheet_name=sheet)
                    if len(test_df) > max_rows:
                        max_rows = len(test_df)
                        data_sheet = sheet
            
            return pd.read_excel(file_path, sheet_name=data_sheet) if data_sheet else pd.read_excel(file_path, sheet_name=sheet_names[0])
        else:
            raise ValueError(f"Unsupported file format")
    
    def preprocess_data(self, df):
        """Preprocess the data"""
        df = df.copy()
        
        # Handle missing values
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        for col in numerical_cols:
            if df[col].isnull().sum() > 0:
                df[col].fillna(df[col].median(), inplace=True)
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].isnull().sum() > 0:
                df[col].fillna(df[col].mode()[0], inplace=True)
        
        # Identify target variable
        target_candidates = ['churn', 'Churn', 'Churned', 'churned', 'is_churn']
        target_col = None
        for col in target_candidates:
            if col in df.columns:
                target_col = col
                break
        
        if target_col is None:
            for col in df.columns:
                if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                    target_col = col
                    break
        
        if target_col is None:
            raise ValueError("Could not identify target variable")
        
        # Separate features and target
        X = df.drop(columns=[target_col])
        y = df[target_col]
        
        # Encode categorical variables
        categorical_cols = X.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            self.label_encoders[col] = le
        
        if y.dtype == 'object':
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
            self.label_encoders['target'] = le_target
        
        return X, y, target_col
    
    def handle_imbalance(self, X, y, method='smote'):
        """Handle imbalanced dataset"""
        unique, counts = np.unique(y, return_counts=True)
        imbalance_ratio = min(counts) / max(counts)
        
        if imbalance_ratio < 0.5:
            if method == 'smote':
                smote = SMOTE(random_state=42)
                X_resampled, y_resampled = smote.fit_resample(X, y)
            else:
                X_resampled, y_resampled = X, y
            return X_resampled, y_resampled
        return X, y
    
    def train_models(self, X_train_scaled, X_test_scaled, y_train, y_test):
        """Train Random Forest and Logistic Regression models with cross-validation"""
        # Define models: Random Forest (target 85%) and Logistic Regression baseline (target 70%)
        models_config = {
            'Logistic Regression': {
                'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
                'params': {
                    'C': [0.1, 0.5, 1.0, 2.0],  # Regularization strength
                    'solver': ['lbfgs', 'liblinear']  # Solver algorithms
                }
            },
            'Random Forest': {
                'model': RandomForestClassifier(random_state=42, class_weight='balanced'),
                'params': {
                    'n_estimators': [150, 200, 250],  # More trees for better accuracy
                    'max_depth': [6, 8, 10],  # Balanced depth for 85% accuracy
                    'min_samples_split': [20, 30, 40],  # Moderate regularization
                    'min_samples_leaf': [10, 15, 20],  # Balanced regularization
                    'max_features': ['sqrt', 'log2']  # Feature diversity
                }
            }
        }
        
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # 3 folds for speed
        target_accuracy_rf = 0.85
        target_accuracy_lr = 0.70
        target_accuracy = 0.87
        results = {}
        
        for name, config in models_config.items():
            # RandomizedSearchCV for faster hyperparameter tuning
            random_search = RandomizedSearchCV(
                config['model'],
                config['params'],
                n_iter=15,  # Test 15 random combinations (faster than full grid)
                cv=cv,
                scoring='accuracy',
                n_jobs=-1,
                verbose=0,
                random_state=42
            )
            random_search.fit(X_train_scaled, y_train)
            best_model = random_search.best_estimator_
            
            # Cross-validation scores
            cv_scores = cross_val_score(
                best_model, X_train_scaled, y_train,
                cv=cv, scoring='accuracy', n_jobs=-1
            )
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            # Train and test predictions
            y_train_pred = best_model.predict(X_train_scaled)
            train_accuracy = accuracy_score(y_train, y_train_pred)
            
            y_pred = best_model.predict(X_test_scaled)
            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]
            test_accuracy = accuracy_score(y_test, y_pred)
            
            overfitting_gap = train_accuracy - test_accuracy
            
            results[name] = {
                'model': best_model,
                'accuracy': test_accuracy,
                'train_accuracy': train_accuracy,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'overfitting_gap': overfitting_gap,
                'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                'f1': f1_score(y_test, y_pred, average='weighted', zero_division=0),
                'roc_auc': roc_auc_score(y_test, y_pred_proba),
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba,
                'best_params': random_search.best_params_
            }
        
        # Compare models and select Random Forest as best
        best_model_name = 'Random Forest'
        self.model = results[best_model_name]['model']
        self.y_pred = results[best_model_name]['y_pred']
        self.y_pred_proba = results[best_model_name]['y_pred_proba']
        self.results = results
        self.best_model_name = best_model_name
        
        # Save models and preprocessing components
        try:
            # Save Random Forest model
            rf_model_path = 'random_forest_model.pkl'
            joblib.dump(results['Random Forest']['model'], rf_model_path)
            
            # Save Logistic Regression model
            lr_model_path = 'logistic_regression_model.pkl'
            joblib.dump(results['Logistic Regression']['model'], lr_model_path)
            
            # Save scaler (if available)
            if hasattr(self, 'scaler') and self.scaler is not None:
                scaler_path = 'scaler.pkl'
                joblib.dump(self.scaler, scaler_path)
            
            # Save PCA (if used)
            if hasattr(self, 'pca') and self.pca is not None:
                pca_path = 'pca.pkl'
                joblib.dump(self.pca, pca_path)
            
            # Save label encoders (if available)
            if hasattr(self, 'label_encoders') and self.label_encoders:
                encoders_path = 'label_encoders.pkl'
                joblib.dump(self.label_encoders, encoders_path)
            
            # Save model metadata
            metadata = {
                'best_model': best_model_name,
                'random_forest': {
                    'accuracy': results['Random Forest']['accuracy'],
                    'roc_auc': results['Random Forest']['roc_auc'],
                    'best_params': results['Random Forest']['best_params']
                },
                'logistic_regression': {
                    'accuracy': results['Logistic Regression']['accuracy'],
                    'roc_auc': results['Logistic Regression']['roc_auc'],
                    'best_params': results['Logistic Regression']['best_params']
                }
            }
            metadata_path = 'model_metadata.pkl'
            joblib.dump(metadata, metadata_path)
        except Exception as e:
            st.warning(f"Could not save models: {e}")
        
        return results, best_model_name


def main():
    """Main application"""
    
    # Header
    st.markdown('<h1 class="main-header">Customer Churn Prediction</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Machine Learning Model for Predicting Customer Churn in E-commerce</p>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Configuration")
        
        # Data source selection
        data_source = st.radio(
            "Data Source",
            ["Download from Kaggle", "Upload File"],
            help="Choose to download from Kaggle or upload your own dataset"
        )
        
        # Imbalance handling method
        imbalance_method = st.selectbox(
            "Imbalance Handling Method",
            ["smote", "smote_tomek", "none"],
            help="Method to handle class imbalance"
        )
        
        st.divider()
        
        # Action buttons
        if st.button("ðŸš€ Train Model", type="primary", use_container_width=True):
            st.session_state.train_model = True
        
        if st.button("ðŸ”„ Reset", use_container_width=True):
            st.session_state.model_trained = False
            st.session_state.data_loaded = False
            st.session_state.predictor = None
            st.rerun()
    
    # Main content area
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š Overview", "ðŸ” Data Analysis", "ðŸ¤– Model Training", "ðŸ“ˆ Results"])
    
    with tab1:
        st.markdown("### Project Overview")
        st.markdown("""
        This application implements a complete Machine Learning pipeline to predict customer churn for e-commerce businesses.
        
        **Key Features:**
        - Automated data download from Kaggle
        - Data preprocessing and feature engineering
        - Handling imbalanced datasets using SMOTE
        - Multiple ML algorithms (Logistic Regression, Random Forest, Gradient Boosting)
        - Comprehensive model evaluation with visualizations
        
        **Workflow:**
        1. Data Collection & Loading
        2. Data Cleaning & Preprocessing
        3. Feature Engineering
        4. Handling Class Imbalance
        5. Model Training & Evaluation
        6. Results Visualization
        """)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Target Accuracy", "~70%", "Baseline")
        with col2:
            st.metric("ROC-AUC", "Moderate", "Good")
        with col3:
            st.metric("Models Tested", "3", "Algorithms")
    
    with tab2:
        st.markdown("### Data Analysis")
        
        if st.session_state.data_loaded and 'df' in st.session_state:
            df = st.session_state.df
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**Dataset Shape**")
                st.write(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
                
                st.markdown("**Missing Values**")
                missing = df.isnull().sum()
                st.dataframe(missing[missing > 0] if missing.sum() > 0 else pd.DataFrame({"Column": ["No missing values"]}))
            
            with col2:
                st.markdown("**Data Types**")
                st.dataframe(df.dtypes.reset_index().rename(columns={0: "Type"}))
            
            st.markdown("**Dataset Preview**")
            st.dataframe(df.head(10), use_container_width=True)
            
            if 'Churn' in df.columns:
                st.markdown("**Class Distribution**")
                churn_counts = df['Churn'].value_counts()
                fig, ax = plt.subplots(figsize=(8, 6))
                churn_counts.plot(kind='bar', ax=ax, color=['#0066cc', '#ff6b6b'])
                ax.set_title('Churn Distribution', fontsize=14, fontweight='bold')
                ax.set_xlabel('Churn Status', fontsize=12)
                ax.set_ylabel('Count', fontsize=12)
                ax.set_xticklabels(['Not Churn', 'Churn'], rotation=0)
                plt.tight_layout()
                st.pyplot(fig)
        else:
            st.info("ðŸ‘† Please train the model first to view data analysis")
    
    with tab3:
        st.markdown("### Model Training")
        
        if st.session_state.get('train_model', False):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Download/Load Data
            status_text.text("ðŸ“¥ Step 1/6: Downloading/Loading Data...")
            progress_bar.progress(10)
            
            predictor = ChurnPredictor()
            
            if data_source == "Download from Kaggle":
                file_path = predictor.download_data()
                if file_path is None:
                    st.error("Failed to download dataset")
                    st.stop()
            else:
                uploaded_file = st.file_uploader("Upload Dataset", type=['csv', 'xlsx', 'xls'])
                if uploaded_file is None:
                    st.warning("Please upload a file")
                    st.stop()
                file_path = uploaded_file.name
                # Save uploaded file temporarily
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
            
            # Load data
            df = predictor.load_data(file_path)
            st.session_state.df = df
            st.session_state.data_loaded = True
            
            status_text.text("âœ… Step 1/6: Data Loaded Successfully")
            progress_bar.progress(20)
            
            # Step 2: Preprocess
            status_text.text("ðŸ”§ Step 2/6: Preprocessing Data...")
            progress_bar.progress(30)
            X, y, target_col = predictor.preprocess_data(df)
            
            status_text.text("âœ… Step 2/6: Data Preprocessed")
            progress_bar.progress(40)
            
            # Step 3: Handle Imbalance
            status_text.text("âš–ï¸ Step 3/6: Handling Class Imbalance...")
            progress_bar.progress(50)
            if imbalance_method != "none":
                X_balanced, y_balanced = predictor.handle_imbalance(X, y, method=imbalance_method)
            else:
                X_balanced, y_balanced = X, y
            
            status_text.text("âœ… Step 3/6: Class Imbalance Handled")
            progress_bar.progress(60)
            
            # Step 4: Split, Scale, and Apply PCA
            status_text.text("ðŸ“Š Step 4/6: Splitting, Scaling & Applying PCA...")
            progress_bar.progress(70)
            X_train, X_test, y_train, y_test = train_test_split(
                X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
            )
            X_train_scaled = predictor.scaler.fit_transform(X_train)
            X_test_scaled = predictor.scaler.transform(X_test)
            
            # Apply PCA
            if X_train_scaled.shape[1] > 2:
                predictor.pca = PCA(n_components=0.95, random_state=42)
                X_train_scaled = predictor.pca.fit_transform(X_train_scaled)
                X_test_scaled = predictor.pca.transform(X_test_scaled)
                predictor.n_components = predictor.pca.n_components_
                predictor.explained_variance_ratio_ = predictor.pca.explained_variance_ratio_.sum()
                predictor.use_pca = True
            else:
                predictor.use_pca = False
            
            predictor.X_test = X_test
            predictor.y_test = y_test
            
            status_text.text("âœ… Step 4/6: Data Split and Scaled")
            progress_bar.progress(80)
            
            # Step 5: Train Models
            status_text.text("ðŸ¤– Step 5/6: Training Models...")
            progress_bar.progress(90)
            results, best_model_name = predictor.train_models(X_train_scaled, X_test_scaled, y_train, y_test)
            
            status_text.text("âœ… Step 5/6: Models Trained")
            progress_bar.progress(100)
            
            st.session_state.predictor = predictor
            st.session_state.model_trained = True
            st.session_state.train_model = False
            
            st.success("âœ… Model Training Completed Successfully!")
            st.balloons()
            
            # Display PCA information
            if predictor.use_pca:
                st.markdown("### PCA Dimensionality Reduction")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Original Features", f"{X.shape[1]}")
                with col2:
                    st.metric("Reduced Features", f"{predictor.n_components}")
                with col3:
                    st.metric("Variance Retained", f"{predictor.explained_variance_ratio_*100:.2f}%")
                st.success(f"âœ“ Features reduced from {X.shape[1]} to {predictor.n_components} while retaining {predictor.explained_variance_ratio_*100:.2f}% variance")
            
            # Display Random Forest model results
            st.markdown("### Random Forest Model Results")
            rf_result = results['Random Forest']
            comparison_df = pd.DataFrame({
                'Metric': ['Test Accuracy', 'Train Accuracy', 'CV Mean', 'CV Std', 'Overfitting Gap', 
                          'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
                'Value': [
                    f"{rf_result['accuracy']:.4f}",
                    f"{rf_result['train_accuracy']:.4f}",
                    f"{rf_result['cv_mean']:.4f}",
                    f"{rf_result['cv_std']:.4f}",
                    f"{rf_result['overfitting_gap']:.4f}",
                    f"{rf_result['precision']:.4f}",
                    f"{rf_result['recall']:.4f}",
                    f"{rf_result['f1']:.4f}",
                    f"{rf_result['roc_auc']:.4f}"
                ]
            })
            st.dataframe(comparison_df, use_container_width=True, hide_index=True)
            
            best_result = results[best_model_name]
            pca_info = f"\n- PCA: {X.shape[1]} â†’ {predictor.n_components} features ({predictor.explained_variance_ratio_*100:.2f}% variance)" if predictor.use_pca else ""
            st.info(f"""
            ðŸ† **Best Model:** {best_model_name}
            - Test Accuracy: {best_result['accuracy']:.2%}
            - CV Mean Accuracy: {best_result['cv_mean']:.2%} (Â±{best_result['cv_std']*2:.2%})
            - Overfitting Gap: {best_result['overfitting_gap']:.4f}
            - ROC-AUC: {best_result['roc_auc']:.4f}{pca_info}
            """)
            
            # Overfitting warning
            if best_result['overfitting_gap'] > 0.10:
                st.warning("âš ï¸ Warning: Significant overfitting detected (gap > 10%)")
            elif best_result['overfitting_gap'] > 0.05:
                st.warning("âš ï¸ Caution: Moderate overfitting detected (gap > 5%)")
            else:
                st.success("âœ“ Good generalization - minimal overfitting detected")
        else:
            st.info("ðŸ‘† Click 'Train Model' button in the sidebar to start training")
    
    with tab4:
        st.markdown("### Model Results & Visualizations")
        
        if st.session_state.model_trained and st.session_state.predictor:
            predictor = st.session_state.predictor
            
            # Get best model results
            best_result = predictor.results[predictor.best_model_name]
            accuracy = best_result['accuracy']
            train_accuracy = best_result['train_accuracy']
            cv_mean = best_result['cv_mean']
            cv_std = best_result['cv_std']
            overfitting_gap = best_result['overfitting_gap']
            precision = best_result['precision']
            recall = best_result['recall']
            f1 = best_result['f1']
            roc_auc = best_result['roc_auc']
            
            st.markdown(f"### Selected Model: **{predictor.best_model_name}**")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("Test Accuracy", f"{accuracy:.2%}", f"{abs(accuracy - 0.87)*100:.2f}% from target")
            with col2:
                st.metric("Precision", f"{precision:.2%}")
            with col3:
                st.metric("Recall", f"{recall:.2%}")
            with col4:
                st.metric("F1-Score", f"{f1:.2%}")
            with col5:
                st.metric("ROC-AUC", f"{roc_auc:.4f}")
            
            st.divider()
            
            # Cross-validation and overfitting metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Train Accuracy", f"{train_accuracy:.2%}")
            with col2:
                st.metric("CV Mean Accuracy", f"{cv_mean:.2%}", f"Â±{cv_std*2:.2%}")
            with col3:
                gap_color = "normal" if overfitting_gap < 0.05 else "inverse"
                st.metric("Overfitting Gap", f"{overfitting_gap:.4f}", delta=None, delta_color=gap_color)
            
            # Overfitting analysis
            if overfitting_gap < 0.02:
                st.success("âœ“ Excellent: No significant overfitting detected")
            elif overfitting_gap < 0.05:
                st.success("âœ“ Good: Minimal overfitting (acceptable)")
            elif overfitting_gap < 0.10:
                st.warning("âš ï¸ Caution: Moderate overfitting detected")
            else:
                st.error("âš ï¸ Warning: Significant overfitting detected")
            
            # CV stability
            if cv_std < 0.01:
                st.success("âœ“ Excellent: Very stable model (CV std < 1%)")
            elif cv_std < 0.02:
                st.success("âœ“ Good: Stable model (CV std < 2%)")
            else:
                st.warning("âš ï¸ Caution: Model variability detected (CV std >= 2%)")
            
            st.divider()
            
            # Confusion Matrix
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### Confusion Matrix")
                cm = confusion_matrix(predictor.y_test, predictor.y_pred)
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=['Not Churn', 'Churn'],
                           yticklabels=['Not Churn', 'Churn'], ax=ax)
                ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
                ax.set_ylabel('Actual', fontsize=12)
                ax.set_xlabel('Predicted', fontsize=12)
                plt.tight_layout()
                st.pyplot(fig)
                
                st.markdown("""
                **Explanation:**
                - **TN (True Negatives):** Correctly predicted non-churn customers
                - **FP (False Positives):** Incorrectly flagged as churn
                - **FN (False Negatives):** Missed churn customers
                - **TP (True Positives):** Correctly predicted churn customers
                """)
            
            with col2:
                st.markdown("### ROC Curve")
                fpr, tpr, _ = roc_curve(predictor.y_test, predictor.y_pred_proba)
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.plot(fpr, tpr, color='darkorange', lw=2, 
                       label=f'ROC curve (AUC = {roc_auc:.4f})')
                ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                       label='Random Classifier (AUC = 0.50)')
                ax.set_xlim([0.0, 1.0])
                ax.set_ylim([0.0, 1.05])
                ax.set_xlabel('False Positive Rate', fontsize=12)
                ax.set_ylabel('True Positive Rate', fontsize=12)
                ax.set_title('ROC Curve', fontsize=14, fontweight='bold')
                ax.legend(loc="lower right", fontsize=10)
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
                
                st.markdown(f"""
                **AUC Score:** {roc_auc:.4f}
                
                **Interpretation:**
                - AUC > 0.9: Excellent
                - AUC > 0.8: Good
                - AUC > 0.7: Moderate
                - AUC < 0.7: Needs improvement
                """)
            
            # Classification Report
            st.markdown("### Classification Report")
            report = classification_report(predictor.y_test, predictor.y_pred, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df, use_container_width=True)
        else:
            st.info("ðŸ‘† Please train the model first to view results")


if __name__ == "__main__":
    main()

